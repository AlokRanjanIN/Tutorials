{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd25cd8-741b-4b68-a2ac-651a116c9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c387253c-3e71-445d-88c6-692d2eff5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, init_method=\"xavier\"):\n",
    "    \"\"\"\n",
    "    Initializes the parameters of the neural network.\n",
    "\n",
    "    Arguments:\n",
    "    layer_dims -- list containing the dimensions of each layer (including input layer)\n",
    "    init_method -- string specifying the initialization method,\n",
    "                   can be \"random\", \"xavier\" or \"he\"\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing the initialized parameters\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # no. of layers + 1 (1 for input layer)\n",
    "    # Loop from l = 1 to L-1\n",
    "    for l in range(1, L):\n",
    "        if init_method == \"random\":\n",
    "            parameters[f\"W_{l}\"] = (\n",
    "                np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "            )\n",
    "        elif init_method == \"xavier\":\n",
    "            parameters[f\"W_{l}\"] = np.random.randn(\n",
    "                layer_dims[l], layer_dims[l - 1]\n",
    "            ) / np.sqrt(layer_dims[l - 1])\n",
    "        elif init_method == \"he\":\n",
    "            parameters[f\"W_{l}\"] = np.random.randn(\n",
    "                layer_dims[l], layer_dims[l - 1]\n",
    "            ) * np.sqrt(2.0 / layer_dims[l - 1])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid initialization method: {init_method}\")\n",
    "\n",
    "        parameters[f\"b_{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0705262-1145-407b-b6d5-13336966f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of Z element-wise.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sigmoid function with respect to Z.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    dZ -- derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    sig_Z = sigmoid(Z)\n",
    "    return sig_Z * (1 - sig_Z)\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer with sigmoid activation.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activated output A\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    dZ = dA * sigmoid_derivative(Z)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Computes the ReLU (Rectified Linear Unit) of Z element-wise.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the ReLU function\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the ReLU function with respect to Z.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    dZ -- derivative of the ReLU function\n",
    "    \"\"\"\n",
    "    return np.where(z <= 0, 0, 1)\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer with ReLU activation.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activated output A\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    dZ = dA * relu_derivative(Z)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def leaky_relu(Z, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Computes the Leaky ReLU of Z element-wise.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "    alpha -- slope of the negative part (default is 0.01)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the Leaky ReLU function\n",
    "    \"\"\"\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "\n",
    "def leaky_relu_derivative(Z, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the Leaky ReLU function with respect to Z.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "    alpha -- slope of the negative part (default is 0.01)\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the Leaky ReLU function\n",
    "    \"\"\"\n",
    "    return np.where(Z > 0, 1, alpha)\n",
    "\n",
    "\n",
    "def leaky_relu_backward(dA, Z, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer with Leaky ReLU activation.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activated output A\n",
    "    Z -- input array\n",
    "    alpha -- slope of the negative part (default is 0.01)\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    return np.where(Z > 0, dA, alpha * dA)\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Computes the hyperbolic tangent of Z element-wise.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the tanh function\n",
    "    \"\"\"\n",
    "    return np.tanh(Z)\n",
    "\n",
    "\n",
    "def tanh_derivative(Z):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the tanh function with respect to Z.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    dZ -- derivative of the tanh function\n",
    "    \"\"\"\n",
    "    tanh_Z = tanh(Z)\n",
    "    return 1 - np.square(tanh_Z)\n",
    "\n",
    "\n",
    "def tanh_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer with tanh activation.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activated output A\n",
    "    Z -- input array\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    dZ = dA * (1 - np.square(tanh(Z)))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ac4f3e-f465-4081-870f-3609a5f217ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Computes the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data), shape (size of previous layer, number of examples)\n",
    "    W -- weights matrix, shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, shape (size of current layer, 1)\n",
    "    activation -- string representing the activation function to use in this layer:\n",
    "                    \"sigmoid\", \"tanh\", \"relu\", or \"leaky_relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function (post-activation value)\n",
    "    cache -- tuple containing \"A_prev\", \"Z\" (linear part), \"W\", and \"b\"\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        A = leaky_relu(Z)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid activation: {activation}\")\n",
    "\n",
    "    cache = (A_prev, Z, W, b)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597c870b-339d-452b-aa96-436d357ce979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(\n",
    "    X, parameters, last_activation=\"sigmoid\", hidden_activation=\"relu\", keep_probs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward propagation for the deep neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (input size, number of examples)\n",
    "    parameters -- dictionary containing the parameters (output of initialization function)\n",
    "    last_activation -- string representing the activation function to use in the last layer (default is \"sigmoid\")\n",
    "    hidden_activation -- string representing the activation function to use in the hidden layers (default is \"relu\")\n",
    "    keep_probs -- list of probabilities for dropout (optional)\n",
    "\n",
    "    Returns:\n",
    "    A_L -- the output of the last activation (post-activation value)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    # Loop from l = 1 to L-1 (with activation == 'relu')\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_forward(\n",
    "            A_prev,\n",
    "            parameters[f\"W_{l}\"],\n",
    "            parameters[f\"b_{l}\"],\n",
    "            activation=hidden_activation,\n",
    "        )\n",
    "        # Apply dropout\n",
    "        if keep_probs is not None:\n",
    "            D = np.random.rand(A.shape[0], A.shape[1]) < keep_probs[l - 1]\n",
    "            A = np.multiply(A, D)\n",
    "            A /= keep_probs[l - 1]\n",
    "        caches.append(cache)\n",
    "    # for l = L (with activation == 'sigmoid')\n",
    "    A_L, cache = linear_forward(\n",
    "        A, parameters[f\"W_{L}\"], parameters[f\"b_{L}\"], activation=last_activation\n",
    "    )\n",
    "    caches.append(cache)\n",
    "    return A_L, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7351a2f5-271b-4055-93d0-b9d46d61f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(A_L, Y, regularization=None, lambd1=0, lambd2=0):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost for binary classification with optional regularization.\n",
    "\n",
    "    Arguments:\n",
    "    A_L -- probability vector corresponding to the label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (1 for cat, 0 for non-cat), shape (1, number of examples)\n",
    "    regularization -- type of regularization: None, \"L1\", \"L2\", \"L1_L2\"\n",
    "    lambd1 -- L1 regularization hyperparameter\n",
    "    lambd2 -- L2 regularization hyperparameter\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost with optional regularization\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8  # Small value to prevent log(0)\n",
    "    m = Y.shape[1]\n",
    "    cross_entropy_cost = (\n",
    "        -np.sum(Y * np.log(A_L + epsilon) + (1 - Y) * (np.log(1 - A_L + epsilon))) / m\n",
    "    )\n",
    "    if regularization is None:\n",
    "        cost = cross_entropy_cost\n",
    "    elif \"L1\" in regularization:\n",
    "        L1_regularization = (lambd1 / (2 * m)) * np.sum(\n",
    "            np.abs([parameters[f\"W{l}\"] for l in range(1, L + 1)])\n",
    "        )\n",
    "        cost = cross_entropy_cost + L1_regularization\n",
    "    elif \"L2\" in regularization:\n",
    "        L2_regularization = (lambd2 / (2 * m)) * np.sum(\n",
    "            np.square([parameters[f\"W{l}\"] for l in range(1, L + 1)])\n",
    "        )\n",
    "        cost = cross_entropy_cost + L2_regularization\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e610675-b2b0-4dae-ad32-d4f1cf5a8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dA, A_prev, Z, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implements the linear portion of backward propagation for one layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activation of the current layer\n",
    "    A_prev -- activations from previous layer (or input data), shape (size of previous layer, number of examples)\n",
    "    Z -- the linear part of the layer's forward propagation\n",
    "    W -- weights matrix, shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, shape (size of current layer, 1)\n",
    "    activation -- string representing the activation function used in this layer:\n",
    "                    \"sigmoid\", \"tanh\", \"relu\", or \"leaky_relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the activation of the previous layer\n",
    "    dW -- gradient of the cost with respect to W (current layer's weights)\n",
    "    db -- gradient of the cost with respect to b (current layer's bias)\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, Z)\n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        dZ = leaky_relu_backward(dA, Z)\n",
    "    else:\n",
    "        raise ValueError(f\"Activation {activation} not supported\")\n",
    "\n",
    "    m = A_prev.shape[1]\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facc16be-80fa-4392-877d-aa7080d4897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(\n",
    "    A_L,\n",
    "    Y,\n",
    "    caches,\n",
    "    last_activation=\"sigmoid\",\n",
    "    hidden_activation=\"relu\",\n",
    "    regularization=None,\n",
    "    lambd1=0,\n",
    "    lambd2=0,\n",
    "    keep_probs = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the entire neural network.\n",
    "\n",
    "    Arguments:\n",
    "    A_L -- the output of the last activation (post-activation value)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_forward() with \"hidden_activation\" (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_forward() with \"last_activation\" (at index L-1)\n",
    "    last_activation -- string representing the activation function used in the last layer (default is \"sigmoid\")\n",
    "    hidden_activation -- string representing the activation function used in the hidden layers (default is \"relu\")\n",
    "    hidden_activation -- the activation for the hidden layers, a string: \"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\"\n",
    "    regularization -- type of regularization: None, \"L1\", \"L2\", \"L1_L2\"\n",
    "    lambd1 -- L1 regularization hyperparameter\n",
    "    lambd2 -- L2 regularization hyperparameter\n",
    "    keep_probs -- \n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary containing the gradients with respect to each parameter\n",
    "             grads[\"dA_L\"] = ...\n",
    "             grads[\"dW_L\"] = ...\n",
    "             grads[\"db_L\"] = ...\n",
    "             ...\n",
    "             grads[\"dW_1\"] = ...\n",
    "             grads[\"db_1\"] = ...\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8  # Small value to prevent division by zero\n",
    "    grads = {}\n",
    "    L = len(caches)  # the number of layers\n",
    "    Y = Y.reshape(A_L.shape)  # after this line, Y is the same shape as A_L\n",
    "    dA_L = -np.divide(Y, A_L + epsilon) + np.divide(1 - Y, 1 - A_L + epsilon)\n",
    "    # loop from l = L to 1\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        A_prev, Z, W, b = caches[l - 1]\n",
    "        if l == L:\n",
    "            dA = dA_L\n",
    "        else:\n",
    "            dA = dA_prev\n",
    "        dA_prev, dW, db = linear_backward(dA, A_prev, Z, W, b, activation=last_activation if l == L else hidden_activation)\n",
    "\n",
    "        # Apply dropout if keep_probs is provided and not for the last layer\n",
    "        if keep_probs is not None and l > 1:\n",
    "            cache, D = caches[l - 1]\n",
    "            dA_prev = np.multiply(dA_prev, D)\n",
    "            dA_prev /= keep_probs[l - 2]  # Scaling\n",
    "            \n",
    "        grads[f\"dW_{l}\"] = dW\n",
    "        grads[f\"db_{l}\"] = db\n",
    "\n",
    "        # Regularization gradient\n",
    "        if regularization is None:\n",
    "            pass\n",
    "        elif \"L1\" in regularization:\n",
    "            grads[f\"dW{l}\"] += (lambd1 / m) * np.sign(grads[f\"dW{l}\"])\n",
    "        elif \"L2\" in regularization:\n",
    "            grads[f\"dW{l}\"] += (lambd2 / m) * parameters[f\"W{l}\"]\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe97785-29ff-4916-acf5-43222e966bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing parameters:\n",
    "                    parameters['W_l'] = weight matrix of shape (size of current layer, size of previous layer)\n",
    "                    parameters['b_l'] = bias vector of shape (size of current layer, 1)\n",
    "    grads -- dictionary containing gradients:\n",
    "                    grads['dW_l'] = gradient of the cost with respect to weight matrix W for layer l\n",
    "                    grads['db_l'] = gradient of the cost with respect to bias vector b for layer l\n",
    "    learning_rate -- learning rate for gradient descent update\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(parameters)\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W_{l}\"] -= learning_rate * grads[f\"dW_{l}\"]\n",
    "        parameters[f\"b_{l}\"] -= learning_rate * grads[f\"db_{l}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18118510-7d98-4665-9f1d-78f83a54687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(\n",
    "    X,\n",
    "    Y,\n",
    "    layers_dims,\n",
    "    optimizer=\"adam\",\n",
    "    learning_rate=0.0075,\n",
    "    n_iters=2500,\n",
    "    regularization=None,\n",
    "    lambd1=0, \n",
    "    lambd2=0,\n",
    "    keep_probs=None,\n",
    "    print_cost=False,\n",
    "    print_cost_iters=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements a deep neural network model.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), shape (1, number of examples)\n",
    "    layer_dims -- list containing the input size and each layer size, length is the number of layers + 1\n",
    "    optimizer -- optimization algorithm: \"gd\", \"adam\"\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    n_iters -- number of iterations\n",
    "    #lambd -- regularization hyperparameter\n",
    "    keep_probs -- list of probabilities for dropout for each layer, if None, dropout is not used\n",
    "    print_cost -- True to print the cost every print_cost_iters iterations\n",
    "    print_cost_iters -- number of iterations between printing the cost\n",
    "\n",
    "    Returns:\n",
    "    parameters -- final parameters learned by the model\n",
    "    costs -- list of costs over training\n",
    "    \"\"\"\n",
    "    costs = []  # keep track of cost\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    for i in range(1, n_iters + 1):\n",
    "        A_L, caches = forward_prop(X, parameters)\n",
    "        cost = get_cost(A_L, Y, regularization=None, lambd1=0, lambd2=0)\n",
    "        grads = backward_prop(A_L, Y, caches, regularization=regularization, lambd1=lambd1, lambd2=lambd2, keep_probs=keep_probs)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if i % 100 == 0 or i == n_iters:\n",
    "            costs.append(cost)\n",
    "        if print_cost and (i % print_cost_iters == 0 or i == n_iters):\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36817dd2-fbd5-4a4b-b005-37996421c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Predicts the labels for given input data using the learned parameters.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data of shape (input size, number of examples)\n",
    "    parameters -- dictionary containing the learned parameters\n",
    "\n",
    "    Returns:\n",
    "    predictions -- array of predictions (0 or 1)\n",
    "    \"\"\"\n",
    "    A_L, _ = forward_prop(X, parameters)\n",
    "    predictions = (A_L > 0.5).astype(int)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def accuracy(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the predictions.\n",
    "\n",
    "    Arguments:\n",
    "    Y_true -- true labels (ground truth)\n",
    "    Y_pred -- predicted labels\n",
    "\n",
    "    Returns:\n",
    "    accuracy -- percentage of correct predictions\n",
    "    \"\"\"\n",
    "    accuracy = np.mean(Y_pred == Y_true) * 100\n",
    "    return accuracy\n",
    "\n",
    "def plot_costs(costs):\n",
    "    \"\"\"\n",
    "    Plots the cost function over iterations.\n",
    "\n",
    "    Arguments:\n",
    "    costs -- list of costs over iterations\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))  # Set the size of the figure\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"Iterations (per hundreds)\")\n",
    "    plt.title(\"Cost vs. Iterations\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd77cc54-0848-4510-9923-75ed294fa66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    \"\"\"\n",
    "    Prints mislabeled images along with their predictions and true labels.\n",
    "\n",
    "    Arguments:\n",
    "    classes -- array of class labels\n",
    "    X -- input data of shape (features, number of examples)\n",
    "    y -- true labels of shape (1, number of examples)\n",
    "    p -- predicted labels of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate indices of mislabeled images\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    \n",
    "    num_images = len(mislabeled_indices[0])\n",
    "\n",
    "    # Display mislabeled images with their predictions and true labels\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        # Configure subplot\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "                \n",
    "        # Display image with increased height and width\n",
    "        plt.imshow(X[:, index].reshape(64, 64, 3), interpolation=\"nearest\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Set title\n",
    "        plt.title(\n",
    "            f\"Pred.: {classes[int(p[0,index])].decode('utf-8')}\\n Class: {classes[y[0,index]].decode('utf-8')}\",\n",
    "            fontsize = 5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5dd314-b0c0-4194-a2eb-2f801c9eaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, image_size):\n",
    "    \"\"\"\n",
    "    Preprocesses an image for classification.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the image file.\n",
    "    image_size (tuple): The target size of the image after resizing.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The preprocessed image as a flattened array.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize(image_size)\n",
    "    img_array = np.array(img)\n",
    "    img_flattened = img_array.reshape((1, -1)).T / 255.0\n",
    "    return img_flattened\n",
    "\n",
    "def show_image(image_path):\n",
    "    \"\"\"\n",
    "    Displays an image.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the image file.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def classify_image(image_path, model_parameters, image_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Classifies an image as \"Cat\" or \"Not Cat\".\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the image file.\n",
    "    model_parameters (dict): Parameters of the trained model.\n",
    "    image_size (tuple): The target size of the image after resizing.\n",
    "\n",
    "    Returns:\n",
    "    str: The predicted class label (\"Cat\" or \"Not Cat\").\n",
    "    \"\"\"\n",
    "    show_image(image_path)\n",
    "    img = preprocess_image(image_path, image_size)\n",
    "    prediction = predict(img, model_parameters)\n",
    "    if prediction == 1:\n",
    "        return \"Cat\"\n",
    "    else:\n",
    "        return \"Not Cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1012c66-995d-4876-b9b2-5eb81d5759ac",
   "metadata": {},
   "source": [
    "## Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6857be-d3af-48be-b9cd-1d6e26fefd78",
   "metadata": {},
   "source": [
    "Alok Ranjan\n",
    "\n",
    "[GitHub Profile](https://www.linkedin.com/in/alokranjan-in/)\n",
    "\n",
    " [LinkedIn Profile](https://github.com/AlokRanjanIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
